{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfPiMaMNBxkL"
      },
      "source": [
        "Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G3SUTS4BxkQ",
        "outputId": "ea865f45-21d6-4524-ae23-d59216f4735e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kxXeq5teBxkS"
      },
      "outputs": [],
      "source": [
        "text1 = 'Zahra is going to the Tehran!?'\n",
        "text2 = 'https://www.w3schools.com'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-RBG51uEBxkS"
      },
      "outputs": [],
      "source": [
        "tokenize1 = nltk.word_tokenize(text1)\n",
        "tokenize2 = nltk.word_tokenize(text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Psx1OcXaBxkS",
        "outputId": "1f1068b5-b997-4788-e3d1-a29616c7ecd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens1: ['Zahra', 'is', 'going', 'to', 'the', 'Tehran', '!', '?']\n",
            "Tokens1: ['https', ':', '//www.w3schools.com']\n"
          ]
        }
      ],
      "source": [
        "print(\"Tokens1:\", tokenize1)\n",
        "print(\"Tokens1:\", tokenize2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8hZtx5OBxkT"
      },
      "source": [
        "Lowercasing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j6LT6OKUBxkU"
      },
      "outputs": [],
      "source": [
        "lowletter = [token.lower() for token in tokenize1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_ycAgnqBxkU",
        "outputId": "151b18cc-5244-4993-b91f-eeabe79598ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['zahra', 'is', 'going', 'to', 'the', 'tehran', '!', '?']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "lowletter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVWsKwXlBxkU"
      },
      "source": [
        "Remove punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "smGG8qBxBxkV"
      },
      "outputs": [],
      "source": [
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-GrVytgSBxkV"
      },
      "outputs": [],
      "source": [
        "remove_punc = [token for token in tokenize1 if token not in string.punctuation ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaN3RxW1BxkV",
        "outputId": "146d7089-2616-43ce-fcb1-db313ff563f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens without punctuation: ['Zahra', 'is', 'going', 'to', 'the', 'Tehran']\n"
          ]
        }
      ],
      "source": [
        "print(\"Tokens without punctuation:\", remove_punc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb9XoYNyBxkW"
      },
      "source": [
        "Remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords as stop\n",
        "\n",
        "stopwords = stop.words(\"english\")\n",
        "\n",
        "print(stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJOpDSS6G6Hp",
        "outputId": "e3f10adc-dd52-40f9-a7b4-fe0111112206"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FwXA_PXXBxkW"
      },
      "outputs": [],
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KkQFt6UoBxkX"
      },
      "outputs": [],
      "source": [
        "remove_stopwords = [token for token in tokenize1 if token not in stopwords]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUfLB0BZBxkX",
        "outputId": "c7f9d68e-8ce4-494d-8b91-3ec1382bbdbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens without stopwords: ['Zahra', 'going', 'Tehran', '!', '?']\n"
          ]
        }
      ],
      "source": [
        "print(\"Tokens without stopwords:\", remove_stopwords)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5cqgFjQBxkX"
      },
      "source": [
        "Remove URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KAZ0OJ7GBxkX"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "G84aJiz0BxkY"
      },
      "outputs": [],
      "source": [
        "# define a regular expression pattern to match URLs\n",
        "pattern = r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\"\n",
        "\n",
        "# replace URLs with an empty string\n",
        "new_text2 = re.sub(pattern, \"\", text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMeLT23SBxkY",
        "outputId": "a28980c6-9789-408a-fb99-f22a5b7c854a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text without URLs: \n"
          ]
        }
      ],
      "source": [
        "print(\"Text without URLs:\", new_text2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Nxwxb_rBxkZ"
      },
      "source": [
        "Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bHoKSsSLBxkZ"
      },
      "outputs": [],
      "source": [
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokenize1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7NkL1WUBxka",
        "outputId": "88963f8c-dea8-477e-f115-cbd18fbf71a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed tokens: ['zahra', 'is', 'go', 'to', 'the', 'tehran', '!', '?']\n"
          ]
        }
      ],
      "source": [
        "print(\"Stemmed tokens:\", stemmed_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "HeKfsxdSyYu_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NMgYL9RFBxka"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Init the Wordnet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibtoylJ5Bxkb",
        "outputId": "a58643d1-5a85-4fe1-a5c8-ae6978708fa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemmatized tokens: ['Zahra', 'is', 'going', 'to', 'the', 'Tehran', '!', '?']\n"
          ]
        }
      ],
      "source": [
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokenize1]\n",
        "print(\"lemmatized tokens:\",lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part-of-speech tagging"
      ],
      "metadata": {
        "id": "pjrT-H0lx3bC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdNIxXhXBxkb",
        "outputId": "94a24c33-0945-4f5c-d5e3-153461f21638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tagged tokens: [('Zahra', 'NNP'), ('is', 'VBZ'), ('going', 'VBG'), ('to', 'TO'), ('the', 'DT'), ('Tehran', 'NN'), ('!', '.'), ('?', '.')]\n"
          ]
        }
      ],
      "source": [
        "# tag the tokens with their POS tags\n",
        "tagged_tokens = nltk.pos_tag(tokenize1)\n",
        "\n",
        "print(\"Tagged tokens:\", tagged_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition"
      ],
      "metadata": {
        "id": "770a6mxeyC36"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfOIQwAPBxkb",
        "outputId": "4cac7101-358c-4033-8f22-bb16900a9f02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named entities: (S\n",
            "  (GPE Zahra/NNP)\n",
            "  is/VBZ\n",
            "  going/VBG\n",
            "  to/TO\n",
            "  the/DT\n",
            "  (ORGANIZATION Tehran/NN)\n",
            "  !/.\n",
            "  ?/.)\n"
          ]
        }
      ],
      "source": [
        "# identify named entities\n",
        "named_entities = nltk.ne_chunk(tagged_tokens)\n",
        "\n",
        "print(\"Named entities:\", named_entities)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}